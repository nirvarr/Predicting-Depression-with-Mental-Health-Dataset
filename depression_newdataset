# required packages
# install.packages(c("tidyverse", "caret", "glmnet", "e1071", "randomForest", 
#                    "rpart", "rpart.plot", "xgboost", "gbm", "gridExtra"))

#setwd("/Users/nirvarroyvaskar/learning/pods")

# loading libraries
library(tidyverse)
library(caret)
library(glmnet)
library(e1071)
library(randomForest)
library(rpart)
library(rpart.plot)
library(xgboost)
library(gbm)
library(gridExtra)
library(readxl)

# loading dataset
df <- read_excel("normalized_mental_health2.xlsx") 
cat("\n\n\n") #printing new lines here

print("We will now COMPUTE various kinds of REGRESSION on our Mental Health dataset")
cat("\n\n\n")

# cleaning the column names from spaces etc. 
names(df) <- make.names(names(df))  


# Select only the specified columns
#selected_columns <- c(
#   "Gender", "Birth.Year", "Ethnic.Group", "Educational.Background", 
#   "Occupation", "Number.of.Children", "Work.Status", "Health.Condition",
#   "Health.Limits.Daily.Activity", "Chronic.Disease", "COVID.19.Infection",
#   "Frequency.of.Washing.Hands", "Frequency.of.Wearing.Facemask",
#   "Support.Government.in.Lockdown", "Support.Government.in.Closing.National.Borders",
#   "Hours.of.Work.Per.Day", "Days.of.Work.in.Usual.Workplace",
#   "Hours.of.Work.From.Home.Per.Day", "Hours.on.Gadgets.Per.Day",
#   "Hours.on.Social.Media.Per.Day",
#   "Hours.of.Exercise.Per.Day", "Anxiety", "Insomnia", "Depression"
# )

#selecting columns with highest correlations to depression
selected_columns <- c(
  "Birth.Year", 
  "Hours.on.Social.Media.Per.Day", 
  "Hours.on.Gadgets.Per.Day",
  "Hours.on.Social.Media..COVID.19.Info..Per.Day",
  "Health.Condition", "Distress",
  "Insomnia", "Anxiety", "Depression"
)

df <- df %>% select(all_of(selected_columns))

# removing a few rows with NA values
df <- na.omit(df)


# converting the Health Condition var from categorical to factors
cat_vars <- c("Health.Condition")
df[cat_vars] <- lapply(df[cat_vars], function(x) {
  x <- as.factor(x)
  x <- factor(make.names(as.character(x)))
  return(x)
}) 

# splitting train-test data with 80:20 ratio
set.seed(123)
index <- createDataPartition(df$Depression, p = 0.8, list = FALSE)
train_raw <- df[index, ]
test_raw <- df[-index, ]

# linear reg
lm_model <- lm(Depression ~ ., data = train_raw)
lm_pred <- predict(lm_model, newdata = test_raw)



# one-hot encoding for categorical variables /nirvar2 
dummy <- dummyVars(" ~ .", data = df)
train <- as.data.frame(predict(dummy, newdata = train_raw))
test <- as.data.frame(predict(dummy, newdata = test_raw))

# some more checking for consistency and avoiding errors
missing_cols <- setdiff(names(train), names(test))
for (col in missing_cols) {
  test[[col]] <- 0
}
test <- test[, names(train)]

# feature matrices 
x_train <- as.matrix(train[, -which(names(train) == "Depression")])
y_train <- train$Depression
x_test <- as.matrix(test[, -which(names(test) == "Depression")])
y_test <- test$Depression

# all the other models except linear(it's up above)
ridge_model <- cv.glmnet(x_train, y_train, alpha = 0)
ridge_pred <- predict(ridge_model, s = "lambda.min", newx = x_test)

lasso_model <- cv.glmnet(x_train, y_train, alpha = 1)
lasso_pred <- predict(lasso_model, s = "lambda.min", newx = x_test)

elastic_model <- cv.glmnet(x_train, y_train, alpha = 0.5)
elastic_pred <- predict(elastic_model, s = "lambda.min", newx = x_test)

tree_model <- rpart(Depression ~ ., data = train, method = "anova")
tree_pred <- predict(tree_model, newdata = test)

# didn't work first rf_model <- randomForest(Depression ~ ., data = train, ntree = 100)
rf_model <- suppressWarnings(randomForest(Depression ~ ., data = train, ntree = 100))

rf_pred <- predict(rf_model, newdata = test)

svr_model <- svm(Depression ~ ., data = train)
svr_pred <- predict(svr_model, newdata = test)

gbm_model <- gbm(Depression ~ ., data = train, distribution = "gaussian",
                 n.trees = 100, interaction.depth = 3, shrinkage = 0.1, cv.folds = 5)
best_iter <- gbm.perf(gbm_model, method = "cv", plot.it = FALSE)
gbm_pred <- predict(gbm_model, newdata = test, n.trees = best_iter)

dtrain <- xgb.DMatrix(data = x_train, label = y_train)
dtest <- xgb.DMatrix(data = x_test)
xgb_model <- xgboost(data = dtrain, nrounds = 100, objective = "reg:squarederror", verbose = 0)
xgb_pred <- predict(xgb_model, newdata = dtest)


# ensuring all predictions are vectors as errors gen while running
ridge_pred   <- as.vector(ridge_pred)
lasso_pred   <- as.vector(lasso_pred)
elastic_pred <- as.vector(elastic_pred)
gbm_pred     <- as.vector(gbm_pred)
xgb_pred     <- as.vector(xgb_pred)

# combining all the preds into a single dataframe /nirv2
reg_results <- data.frame(
  Actual_Depression_Value        = test$Depression,
  Linear        = lm_pred,
  Ridge         = ridge_pred,
  Lasso         = lasso_pred,
  Elastic       = elastic_pred,
  Decision      = tree_pred,
  Random_Forest = rf_pred,
  GBM           = gbm_pred,
  SVR           = svr_pred,
  XGBoost       = xgb_pred
)

# printing the first few test results
print(head(reg_results))



# evaluation equations
rmse <- function(actual, predicted) sqrt(mean((actual - predicted)^2))
mae <- function(actual, predicted) mean(abs(actual - predicted))
r2 <- function(actual, predicted) 1 - sum((actual - predicted)^2) / sum((actual - mean(actual))^2)
mape <- function(actual, predicted) mean(abs((actual - predicted) / actual)) * 100
smape <- function(actual, predicted) mean(2 * abs(predicted - actual) / (abs(actual) + abs(predicted))) * 100
explained_variance <- function(actual, predicted) 1 - var(actual - predicted) / var(actual)


# eval fn

evaluate_model <- function(name, actual, predicted) {
  data.frame(
    Model = name,
    Root_Mean_Square_Error = paste0(round(rmse(actual, predicted) * 100, 2), "%"),
    Mean_Absolute_Error = paste0(round(mae(actual, predicted) * 100, 2), "%"),
    R_squared = paste0(round(r2(actual, predicted) * 100, 2), "%"),
    #doesn't work MAPE = paste0(round(mape(actual, predicted) * 100, 2), "%"),
    #doesn't worl SMAPE = paste0(round(smape(actual, predicted) * 100, 2), "%"),
    Explained_Variance = paste0(round(explained_variance(actual, predicted) * 100, 2), "%")
  )
}


results <- rbind(
  evaluate_model("Linear", y_test, lm_pred),
  evaluate_model("Ridge", y_test, as.vector(ridge_pred)),
  evaluate_model("Lasso", y_test, as.vector(lasso_pred)),
  evaluate_model("Elastic Net", y_test, as.vector(elastic_pred)),
  evaluate_model("Decision Tree", y_test, tree_pred),
  evaluate_model("Random Forest", y_test, rf_pred),
  evaluate_model("GBM", y_test, gbm_pred),
  evaluate_model("SVR", y_test, svr_pred),
  evaluate_model("XGBoost", y_test, xgb_pred)
)

cat("\n\n\n\n\n") #new line for clarity
print("The ERROR METRICS are provided below:")
print(results) 
